{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_DL-application_COVID-19.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ntd-JgF6dfAF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntd-JgF6dfAF"
      },
      "source": [
        "## Package installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnni4XBKKt2W"
      },
      "source": [
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-py37_4.8.3-Linux-x86_64.sh -b -f -p /usr/local\n",
        "#!time conda install -q -y -c conda-forge rdkit\n",
        "!time conda install -c rdkit rdkit\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB-6AKYKL2wh"
      },
      "source": [
        "!pip install --pre deepchem #\n",
        "!pip install dgl-cu110\n",
        "!pip install dgllife"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGZc3HdLitKz"
      },
      "source": [
        "!pip install xgboost\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRiOW2Qtdjxt"
      },
      "source": [
        "## Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrhw_-Nldos6"
      },
      "source": [
        "import deepchem as dc\n",
        "from deepchem.molnet.preset_hyper_parameters import hps\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnBREfovdZNe"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QwXbN0XZg-v"
      },
      "source": [
        "def load_data(model, data):\n",
        "    \n",
        "    if model in [\"graphconv\", \"dag\"]:\n",
        "      featurizer = dc.feat.ConvMolFeaturizer()\n",
        "    elif model in [\"tf\", \"irv\", \"tf_robust\", \"kernelsvm\", \"rf\", \"logreg\", \"xgb\"]:\n",
        "      featurizer = dc.feat.CircularFingerprint()\n",
        "    elif model in [\"gat\", \"gcn\"]:\n",
        "      featurizer = dc.feat.MolGraphConvFeaturizer()\n",
        "    elif model == \"mpnn\":\n",
        "      featurizer = dc.feat.WeaveFeaturizer()\n",
        "    elif model == \"textcnn\":\n",
        "      featurizer = None\n",
        "\n",
        "    tasks = ['label']\n",
        "    loader = dc.data.CSVLoader(tasks=tasks, feature_field=\"smiles\",featurizer=featurizer)\n",
        "    dataset = loader.create_dataset(data)\n",
        "    transformer = None\n",
        "\n",
        "    return (dataset, [transformer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHobkiRxqdhB"
      },
      "source": [
        "data = \"/mydrive/drug_discovery/data.csv\"\n",
        "dataset, transformers = load_data(\"dag\", data)\n",
        "splitter = dc.splits.RandomSplitter()\n",
        "train_dataset, test_dataset = splitter.train_test_split(dataset, frac_train = 0.90, seed=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v97zy3OIZttm"
      },
      "source": [
        "print(\"Dataset size  : \",train_dataset.X.shape[0] + test_dataset.X.shape[0])\n",
        "print(\"Trainset size : \",train_dataset.X.shape[0])\n",
        "print(\"Testset size  : \",test_dataset.X.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRwb4DCSdJz0"
      },
      "source": [
        "# Fit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWAxjyqH0OA6"
      },
      "source": [
        "## Benchmark classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHrFunSsXdA_"
      },
      "source": [
        "def benchmark_classification(train_dataset,\n",
        "                             valid_dataset,\n",
        "                             test_dataset,\n",
        "                             tasks,\n",
        "                             transformers,\n",
        "                             n_features,\n",
        "                             metric,\n",
        "                             model,\n",
        "                             test=False,\n",
        "                             hyper_parameters=None,\n",
        "                             seed=123):\n",
        "  \"\"\"\n",
        "  Calcul la performance de différents modèles sur l'ensemble de données et les tasks spécifiques.\n",
        "  Paramètres\n",
        "  ----------\n",
        "  train_dataset : struct dataset\n",
        "      Jeu de données utilisé pour l'entraînement et l'évaluation du modèle\n",
        "  valid_dataset : struct dataset\n",
        "      jeu de données utilisé uniquement pour l'évaluation du modèle (et le réglage des hyperparamètres)\n",
        "  test_dataset : struct dataset\n",
        "      jeu de données utilisé uniquement pour l'évaluation du modèle\n",
        "  tasks : liste de chaînes de caractères\n",
        "      liste de cibles (tasks, datasets)\n",
        "  transformers : dc.trans.Transformer struct\n",
        "      transformateur utilisé pour l'évaluation du modèle\n",
        "  n_features : integer\n",
        "      nombre de caractéristiques, ou longueur des binary fingerprints\n",
        "  metric : liste d'objets dc.metrics.Metric\n",
        "      métriques utilisées pour l'évaluation\n",
        "  model : chaîne de caractères, facultatif\n",
        "      choix du modèle\n",
        "      rf', 'tf', 'tf_robust', 'logreg', 'irv', 'graphconv', 'dag', 'xgb',\n",
        "      weave', 'kernelsvm', 'textcnn', 'mpnn'.\n",
        "  test : booléen, facultatif\n",
        "      calcul ou non des performances de test_set\n",
        "  hyper_parameters : dict, facultatif (default=None)\n",
        "      paramètres hyper pour le modèle désigné, None = utiliser les valeurs prédéfinies\n",
        "  Retourne\n",
        "  -------\n",
        "  train_scores : dict\n",
        "  résultats de prédiction (AUC) sur l'ensemble d'entraînement\n",
        "  valid_scores : dict\n",
        "  prédiction des résultats (AUC) sur l'ensemble valide\n",
        "  test_scores : dict\n",
        "  prédiction des résultats (AUC) sur l'ensemble de test\n",
        "  \"\"\"\n",
        "  train_scores = {}\n",
        "  valid_scores = {}\n",
        "  test_scores = {}\n",
        "\n",
        "  assert model in [\n",
        "      'rf', 'tf', 'tf_robust', 'logreg', 'irv', 'graphconv', 'dag', 'xgb',\n",
        "      'weave', 'kernelsvm', 'textcnn', 'mpnn', 'gat', 'gcn'\n",
        "  ]\n",
        "  if hyper_parameters is None and model not in ['gat', 'gcn']:\n",
        "    hyper_parameters = hps[model]\n",
        "  model_name = model\n",
        "\n",
        "  if model_name == \"gat\":\n",
        "    nb_epoch = 40\n",
        "    model = dc.models.GATModel(1,\n",
        "                 mode='classification',\n",
        "                 batch_size=32,\n",
        "                 learning_rate=0.0001,\n",
        "                 dropout=0.25,\n",
        "                 )\n",
        "  elif model_name == \"gcn\":\n",
        "    nb_epoch = 40\n",
        "    model = dc.models.GCNModel(1,\n",
        "                 mode='classification',\n",
        "                 batch_size=32,\n",
        "                 learning_rate=0.001,\n",
        "                 dropout=0.1,\n",
        "                 )\n",
        "\n",
        "  elif model_name == 'tf':\n",
        "    layer_sizes = hyper_parameters['layer_sizes']\n",
        "    weight_init_stddevs = hyper_parameters['weight_init_stddevs']\n",
        "    bias_init_consts = hyper_parameters['bias_init_consts']\n",
        "    dropouts = hyper_parameters['dropouts']\n",
        "    penalty = hyper_parameters['penalty']\n",
        "    penalty_type = hyper_parameters['penalty_type']\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "\n",
        "    # Building tensorflow MultitaskDNN model\n",
        "    model = dc.models.MultitaskClassifier(\n",
        "        len(tasks),\n",
        "        2048,\n",
        "        layer_sizes=layer_sizes,\n",
        "        weight_init_stddevs=weight_init_stddevs,\n",
        "        bias_init_consts=bias_init_consts,\n",
        "        dropouts=dropouts,\n",
        "        weight_decay_penalty=penalty,\n",
        "        weight_decay_penalty_type=penalty_type,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed)\n",
        "\n",
        "  elif model_name == 'tf_robust':\n",
        "    layer_sizes = hyper_parameters['layer_sizes']\n",
        "    weight_init_stddevs = hyper_parameters['weight_init_stddevs']\n",
        "    bias_init_consts = hyper_parameters['bias_init_consts']\n",
        "    dropouts = hyper_parameters['dropouts']\n",
        "\n",
        "    bypass_layer_sizes = hyper_parameters['bypass_layer_sizes']\n",
        "    bypass_weight_init_stddevs = hyper_parameters['bypass_weight_init_stddevs']\n",
        "    bypass_bias_init_consts = hyper_parameters['bypass_bias_init_consts']\n",
        "    bypass_dropouts = hyper_parameters['bypass_dropouts']\n",
        "\n",
        "    penalty = hyper_parameters['penalty']\n",
        "    penalty_type = hyper_parameters['penalty_type']\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "\n",
        "    # Building tensorflow robust MultitaskDNN model\n",
        "    model = dc.models.RobustMultitaskClassifier(\n",
        "        len(tasks),\n",
        "        n_features,\n",
        "        layer_sizes=layer_sizes,\n",
        "        weight_init_stddevs=weight_init_stddevs,\n",
        "        bias_init_consts=bias_init_consts,\n",
        "        dropouts=dropouts,\n",
        "        bypass_layer_sizes=bypass_layer_sizes,\n",
        "        bypass_weight_init_stddevs=bypass_weight_init_stddevs,\n",
        "        bypass_bias_init_consts=bypass_bias_init_consts,\n",
        "        bypass_dropouts=bypass_dropouts,\n",
        "        weight_decay_penalty=penalty,\n",
        "        weight_decay_penalty_type=penalty_type,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed)\n",
        "\n",
        "  elif model_name == 'logreg':\n",
        "    penalty = hyper_parameters['penalty']\n",
        "    penalty_type = hyper_parameters['penalty_type']\n",
        "    nb_epoch = None\n",
        "\n",
        "    # Building scikit logistic regression model\n",
        "    def model_builder(model_dir):\n",
        "      sklearn_model = LogisticRegression(\n",
        "          penalty=penalty_type,\n",
        "          C=1. / penalty,\n",
        "          class_weight=\"balanced\",\n",
        "          n_jobs=-1)\n",
        "      return dc.models.sklearn_models.SklearnModel(\n",
        "          sklearn_model, model_dir)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        tasks, model_builder)\n",
        "\n",
        "  elif model_name == 'irv':\n",
        "    penalty = hyper_parameters['penalty']\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_K = hyper_parameters['n_K']\n",
        "\n",
        "    # Transform fingerprints to IRV features\n",
        "    transformer = dc.trans.IRVTransformer(n_K, len(tasks), train_dataset)\n",
        "    train_dataset = transformer.transform(train_dataset)\n",
        "    valid_dataset = transformer.transform(valid_dataset)\n",
        "    if test:\n",
        "      test_dataset = transformer.transform(test_dataset)\n",
        "\n",
        "    # Building tensorflow IRV model\n",
        "    model = dc.models.MultitaskIRVClassifier(\n",
        "        len(tasks),\n",
        "        K=n_K,\n",
        "        penalty=penalty,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'graphconv':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_filters = hyper_parameters['n_filters']\n",
        "    n_fully_connected_nodes = hyper_parameters['n_fully_connected_nodes']\n",
        "\n",
        "    model = dc.models.GraphConvModel(\n",
        "        len(tasks),\n",
        "        graph_conv_layers=[n_filters] * 2,\n",
        "        dense_layer_size=n_fully_connected_nodes,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'dag':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_graph_feat = hyper_parameters['n_graph_feat']\n",
        "    default_max_atoms = hyper_parameters['default_max_atoms']\n",
        "\n",
        "    max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])\n",
        "    max_atoms_valid = max([mol.get_num_atoms() for mol in valid_dataset.X])\n",
        "    if test :\n",
        "      max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])\n",
        "      max_atoms = max([max_atoms_train, max_atoms_valid, max_atoms_test])\n",
        "    else:\n",
        "      max_atoms = max([max_atoms_train, max_atoms_valid])\n",
        "    max_atoms = min([max_atoms, default_max_atoms])\n",
        "    print('Maximum number of atoms: %i' % max_atoms)\n",
        "    reshard_size = 256\n",
        "    transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)\n",
        "    train_dataset.reshard(reshard_size)\n",
        "    train_dataset = transformer.transform(train_dataset)\n",
        "    valid_dataset.reshard(reshard_size)\n",
        "    valid_dataset = transformer.transform(valid_dataset)\n",
        "    if test:\n",
        "      test_dataset.reshard(reshard_size)\n",
        "      test_dataset = transformer.transform(test_dataset)\n",
        "\n",
        "    model = dc.models.DAGModel(\n",
        "        len(tasks),\n",
        "        max_atoms=max_atoms,\n",
        "        n_atom_feat=75,\n",
        "        n_graph_feat=n_graph_feat,\n",
        "        n_outputs=30,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed,\n",
        "        use_queue=False,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'weave':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_graph_feat = hyper_parameters['n_graph_feat']\n",
        "    n_pair_feat = hyper_parameters['n_pair_feat']\n",
        "\n",
        "    model = dc.models.WeaveModel(\n",
        "        len(tasks),\n",
        "        n_atom_feat=n_features,\n",
        "        n_pair_feat=n_pair_feat,\n",
        "        n_hidden=50,\n",
        "        n_graph_feat=n_graph_feat,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        use_queue=False,\n",
        "        random_seed=seed,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'textcnn':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_embedding = hyper_parameters['n_embedding']\n",
        "    filter_sizes = hyper_parameters['filter_sizes']\n",
        "    num_filters = hyper_parameters['num_filters']\n",
        "\n",
        "    all_data = dc.data.DiskDataset.merge(\n",
        "        [train_dataset, valid_dataset, test_dataset])\n",
        "    char_dict, length = dc.models.TextCNNModel.build_char_dict(all_data)\n",
        "\n",
        "    model = dc.models.TextCNNModel(\n",
        "        len(tasks),\n",
        "        char_dict,\n",
        "        seq_length=length,\n",
        "        n_embedding=n_embedding,\n",
        "        filter_sizes=filter_sizes,\n",
        "        num_filters=num_filters,\n",
        "        learning_rate=learning_rate,\n",
        "        batch_size=batch_size,\n",
        "        use_queue=False,\n",
        "        random_seed=seed,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'mpnn':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    T = hyper_parameters['T']\n",
        "    M = hyper_parameters['M']\n",
        "\n",
        "    model = dc.models.MPNNModel(\n",
        "        len(tasks),\n",
        "        n_atom_feat=n_features[0],\n",
        "        n_pair_feat=n_features[1],\n",
        "        n_hidden=n_features[0],\n",
        "        T=T,\n",
        "        M=M,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        use_queue=False,\n",
        "        mode=\"classification\")\n",
        "\n",
        "  elif model_name == 'rf':\n",
        "    n_estimators = hyper_parameters['n_estimators']\n",
        "    nb_epoch = None\n",
        "\n",
        "    # Building scikit random forest model\n",
        "    def model_builder(model_dir):\n",
        "      sklearn_model = RandomForestClassifier(\n",
        "          class_weight=\"balanced\", n_estimators=n_estimators, n_jobs=-1)\n",
        "      return dc.models.sklearn_models.SklearnModel(\n",
        "          sklearn_model, model_dir)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        tasks, model_builder)\n",
        "\n",
        "  elif model_name == 'kernelsvm':\n",
        "    C = hyper_parameters['C']\n",
        "    gamma = hyper_parameters['gamma']\n",
        "    nb_epoch = None\n",
        "\n",
        "    # Building scikit learn Kernel SVM model\n",
        "    def model_builder(model_dir):\n",
        "      sklearn_model = SVC(\n",
        "          C=C, gamma=gamma, class_weight=\"balanced\", probability=True)\n",
        "      return dc.models.SklearnModel(sklearn_model, model_dir)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        tasks, model_builder)\n",
        "\n",
        "  elif model_name == 'xgb':\n",
        "    max_depth = hyper_parameters['max_depth']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_estimators = hyper_parameters['n_estimators']\n",
        "    gamma = hyper_parameters['gamma']\n",
        "    min_child_weight = hyper_parameters['min_child_weight']\n",
        "    max_delta_step = hyper_parameters['max_delta_step']\n",
        "    subsample = hyper_parameters['subsample']\n",
        "    colsample_bytree = hyper_parameters['colsample_bytree']\n",
        "    colsample_bylevel = hyper_parameters['colsample_bylevel']\n",
        "    reg_alpha = hyper_parameters['reg_alpha']\n",
        "    reg_lambda = hyper_parameters['reg_lambda']\n",
        "    scale_pos_weight = hyper_parameters['scale_pos_weight']\n",
        "    base_score = hyper_parameters['base_score']\n",
        "    seed = hyper_parameters['seed']\n",
        "    early_stopping_rounds = hyper_parameters['early_stopping_rounds']\n",
        "    nb_epoch = None\n",
        "\n",
        "    esr = {'early_stopping_rounds': early_stopping_rounds}\n",
        "\n",
        "    # Building xgboost classification model\n",
        "    def model_builder(model_dir):\n",
        "      import xgboost\n",
        "      xgboost_model = xgboost.XGBClassifier(\n",
        "          max_depth=max_depth,\n",
        "          learning_rate=learning_rate,\n",
        "          n_estimators=n_estimators,\n",
        "          gamma=gamma,\n",
        "          min_child_weight=min_child_weight,\n",
        "          max_delta_step=max_delta_step,\n",
        "          subsample=subsample,\n",
        "          colsample_bytree=colsample_bytree,\n",
        "          colsample_bylevel=colsample_bylevel,\n",
        "          reg_alpha=reg_alpha,\n",
        "          reg_lambda=reg_lambda,\n",
        "          scale_pos_weight=scale_pos_weight,\n",
        "          base_score=base_score,\n",
        "          seed=seed)\n",
        "      return dc.models.GBDTModel(\n",
        "          xgboost_model, model_dir, **esr)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        tasks, model_builder)\n",
        "\n",
        "  if nb_epoch is None:\n",
        "    model.fit(train_dataset)\n",
        "  else:\n",
        "    model.fit(train_dataset, nb_epoch=nb_epoch)\n",
        "\n",
        "  train_scores[model_name] = model.evaluate(train_dataset, metric)\n",
        "  valid_scores[model_name] = model.evaluate(valid_dataset, metric)\n",
        "  if test:\n",
        "    test_scores[model_name] = model.evaluate(test_dataset, metric)\n",
        "\n",
        "  return train_scores, valid_scores, test_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVmV0dC60X0H"
      },
      "source": [
        "## Instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NbXgswbgAG4"
      },
      "source": [
        "metric = [dc.metrics.Metric(dc.metrics.roc_auc_score), dc.metrics.Metric(dc.metrics.f1_score), dc.metrics.Metric(dc.metrics.recall_score)]\n",
        "\n",
        "train_scores, test_scores, _valid_scores = benchmark_classification(train_dataset = train_dataset,\n",
        "                                                                    valid_dataset = test_dataset,\n",
        "                                                                    test_dataset = None,\n",
        "                                                                    tasks = [1],\n",
        "                                                                    transformers = transformers,\n",
        "                                                                    n_features = 2048,\n",
        "                                                                    metric = metric,\n",
        "                                                                    model = 'xgb',\n",
        "                                                                    test=False,\n",
        "                                                                    hyper_parameters=None,\n",
        "                                                                    seed=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdMYqUibCEkz"
      },
      "source": [
        "#Hyperparameter Tuning\n",
        "One of the most important aspects of machine learning is hyperparameter tuning. Many machine learning models have a number of hyperparameters that control aspects of the model. These hyperparameters typically cannot be learned directly by the same learning algorithm used for the rest of learning and have to be set in an alternate fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_ZO74t1mFtj"
      },
      "source": [
        "###GCNModel model Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUNPO-h9mLsU"
      },
      "source": [
        "nb_epoch = [120, 40]\n",
        "batch_size = [64, 32, 16]\n",
        "learning_rate = [0.001, 0.0001]\n",
        "dropout = [0.15, 0.10]\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "\n",
        "param_nb_epoch = []\n",
        "param_batch_size = []\n",
        "param_learning_rate =[]\n",
        "param_dropout =[]\n",
        "test_acc = []\n",
        "test_auc = []\n",
        "import pandas as pd\n",
        "parameters_results = pd.DataFrame()\n",
        "\n",
        "for ep in nb_epoch:\n",
        "  for bsiz in batch_size:\n",
        "    for lgr in learning_rate:\n",
        "      for dp in dropout:\n",
        "        print(\"Curent params : \", ep, bsiz, lgr, dp)\n",
        "        print(\"Instanciate model...\\n\")\n",
        "        model = dc.models.GCNModel(1,\n",
        "                 mode='classification',\n",
        "                 batch_size=bsiz,\n",
        "                 learning_rate=lgr,\n",
        "                 dropout=dp,\n",
        "                 )\n",
        "        print(\"Fitting model...\")\n",
        "        model.fit(train_dataset, nb_epoch= ep)\n",
        "        test_score = model.evaluate(test_dataset, [metric])\n",
        "        y_test = test_dataset.y\n",
        "        y_test_pred = model.predict(test_dataset)\n",
        "        y_tst_pred = np.argmax(y_test_pred, axis=1) \n",
        "        y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "        y_test_prediction = y_test_prediction.astype('float64')\n",
        "\n",
        "        param_nb_epoch.append(ep)\n",
        "        param_batch_size.append(bsiz)\n",
        "        param_learning_rate.append(lgr)\n",
        "        param_dropout.append(dp)\n",
        "        test_acc.append(round(accuracy_score(y_test, y_test_prediction), 2))\n",
        "        test_auc.append(round(test_score[\"roc_auc_score\"], 2))\n",
        "\n",
        "  parameters_results[\"epoch\"] = param_nb_epoch\n",
        "  parameters_results[\"dropout\"] = param_dropout\n",
        "  parameters_results[\"learning_rate\"] = param_learning_rate\n",
        "  parameters_results[\"batch_size\"] = param_batch_size\n",
        "  parameters_results[\"accuracy\"] = test_acc\n",
        "  parameters_results[\"auc roc score\"] = test_auc\n",
        "  parameters_results.to_csv(\"/mydrive/drug_discovery/GCNModel_Opt.csv\", index=False)\n",
        "  parameters_results = pd.DataFrame()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULG1CrvZBwIV"
      },
      "source": [
        "### RF Model Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF1UwUahBuAg"
      },
      "source": [
        "model = RandomForestClassifier()\n",
        "n_estimators = [10, 100, 300, 500, 600, 1000]\n",
        "max_features = ['sqrt', 'log2']\n",
        "nb_epoch = None\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "\n",
        "import pandas as pd\n",
        "parameters_results = pd.DataFrame()\n",
        "param_n_estimators = []\n",
        "param_max_features = []\n",
        "test_acc = []\n",
        "test_auc = []\n",
        "\n",
        "for n_est in n_estimators:\n",
        "  for max_f in max_features:\n",
        "    print(\"Curent params : \", n_est, max_f)\n",
        "    print(\"Instanciate model...\")\n",
        "    # Building scikit random forest model\n",
        "    def model_builder(model_dir):\n",
        "      sklearn_model = RandomForestClassifier(\n",
        "          class_weight = \"balanced\", n_estimators = n_est, max_features = max_f, n_jobs=-1)\n",
        "      return dc.models.sklearn_models.SklearnModel(\n",
        "          sklearn_model, model_dir)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        [1], model_builder)\n",
        "    print(\"Fitting model...\\n\")\n",
        "    model.fit(train_dataset)\n",
        "\n",
        "    test_score = model.evaluate(test_dataset, [metric])\n",
        "    y_test = test_dataset.y\n",
        "    y_test_pred = model.predict(test_dataset)\n",
        "    y_tst_pred = np.argmax(y_test_pred[:,0], axis=1) \n",
        "    y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "    y_test_prediction = y_test_prediction.astype('float64')\n",
        "\n",
        "    param_n_estimators.append(n_est)\n",
        "    param_max_features.append(max_f)\n",
        "    test_acc.append(round(accuracy_score(y_test, y_test_prediction), 2))\n",
        "    test_auc.append(round(test_score[\"roc_auc_score\"], 2))\n",
        "\n",
        "  parameters_results[\"n_estimators\"] = param_n_estimators\n",
        "  parameters_results[\"max_features\"] = param_max_features\n",
        "  parameters_results[\"accuracy\"] = test_acc\n",
        "  parameters_results[\"auc roc score\"] = test_auc\n",
        "  parameters_results.to_csv(\"/mydrive/drug_discovery/RF_Opt.csv\", index=False)\n",
        "  parameters_results = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCaDD2JYwkVL"
      },
      "source": [
        "### DAGModel Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8CGBPxDwvjp"
      },
      "source": [
        "batch_size = [64, 32]  #hyper_parameters['batch_size']\n",
        "nb_epoch = [50, 40] #hyper_parameters['nb_epoch']\n",
        "learning_rate = [0.0005, 0.001] #hyper_parameters['learning_rate']\n",
        "n_graph_feat =  [30, 20] #hyper_parameters['n_graph_feat']\n",
        "default_max_atoms = 60 #hyper_parameters['default_max_atoms']\n",
        "seed = 123\n",
        "max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])\n",
        "max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])\n",
        "max_atoms = max([max_atoms_train, max_atoms_test])\n",
        "max_atoms = min([max_atoms, default_max_atoms])\n",
        "print('Maximum number of atoms: %i' % max_atoms)\n",
        "reshard_size = 256\n",
        "transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)\n",
        "train_dataset.reshard(reshard_size)\n",
        "train_dataset = transformer.transform(train_dataset)\n",
        "test_dataset.reshard(reshard_size)\n",
        "test_dataset = transformer.transform(test_dataset)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "\n",
        "parambatch_size = []\n",
        "paramnb_epoch = []\n",
        "paramlearning_rate = []\n",
        "paramn_graph_feat =  []\n",
        "test_acc = []\n",
        "test_auc = []\n",
        "import pandas as pd\n",
        "parameters_results = pd.DataFrame()\n",
        "for btch_s in batch_size:\n",
        "  for nb_ep in nb_epoch:\n",
        "    for lrg_r in learning_rate:\n",
        "      for n_gph in n_graph_feat:\n",
        "        print(\"current conf : \", nb_ep, btch_s, lrg_r, n_gph)\n",
        "        print(\"instantiate the model...\")\n",
        "        model = dc.models.DAGModel(\n",
        "            1,\n",
        "            max_atoms=max_atoms,\n",
        "            n_atom_feat=75,\n",
        "            n_graph_feat=n_gph,\n",
        "            n_outputs=30,\n",
        "            batch_size=btch_s,\n",
        "            learning_rate=lrg_r,\n",
        "            random_seed=seed,\n",
        "            use_queue=False,\n",
        "            mode='classification')\n",
        "        \n",
        "        print(\"Fitting the model...\")\n",
        "        model.fit(train_dataset, nb_epoch= nb_ep)\n",
        "        print(\"evaluate the model...\")\n",
        "        test_score = model.evaluate(test_dataset, [metric])\n",
        "        y_test = test_dataset.y\n",
        "        y_test_pred = model.predict(test_dataset)\n",
        "        y_tst_pred = np.argmax(y_test_pred[:,0], axis=1) \n",
        "        y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "        y_test_prediction = y_test_prediction.astype('float64')\n",
        "\n",
        "        paramnb_epoch.append(nb_ep)\n",
        "        parambatch_size.append(btch_s)\n",
        "        paramlearning_rate.append(lrg_r)\n",
        "        paramn_graph_feat.append(n_gph)\n",
        "        test_acc.append(round(accuracy_score(y_test, y_test_prediction), 2))\n",
        "        test_auc.append(round(test_score[\"roc_auc_score\"], 2))\n",
        "  print(\"save the model...\\n\")\n",
        "  parameters_results[\"epoch\"] = paramnb_epoch\n",
        "  parameters_results[\"n_graph_feat\"] = paramn_graph_feat\n",
        "  parameters_results[\"learning_rate\"] = paramlearning_rate\n",
        "  parameters_results[\"batch_size\"] = parambatch_size\n",
        "  parameters_results[\"accuracy\"] = test_acc\n",
        "  parameters_results[\"auc roc score\"] = test_auc\n",
        "  parameters_results.to_csv(\"/mydrive/drug_discovery/DAGModel_Opt.csv\", index=False)\n",
        "  parameters_results = pd.DataFrame()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTG118GJ9czB"
      },
      "source": [
        "# Load best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVt0kHxFLGeg"
      },
      "source": [
        "### RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVJdUKYDLCqG"
      },
      "source": [
        "def model_builder(model_dir):\n",
        "\n",
        "  sklearn_model = RandomForestClassifier(\n",
        "      class_weight = \"balanced\", n_estimators = 300, max_features = 'sqrt', n_jobs=-1)\n",
        "  return dc.models.sklearn_models.SklearnModel(\n",
        "      sklearn_model, model_dir)\n",
        "\n",
        "model = dc.models.multitask.SingletaskToMultitask([1], model_builder)\n",
        "model.fit(train_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9qugXLbLKSa"
      },
      "source": [
        "### GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdwiIRYGLORo"
      },
      "source": [
        "model = dc.models.GCNModel(1,\n",
        "                mode='classification',\n",
        "                batch_size=64,\n",
        "                learning_rate=0.001,\n",
        "                dropout=0.1,\n",
        "                )\n",
        "model.fit(train_dataset, nb_epoch= 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avqm1NpoLO2F"
      },
      "source": [
        "### DAG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXaAnsLLSES"
      },
      "source": [
        "default_max_atoms = 60 \n",
        "seed = 123\n",
        "max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])\n",
        "max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])\n",
        "max_atoms = max([max_atoms_train, max_atoms_test])\n",
        "max_atoms = min([max_atoms, default_max_atoms])\n",
        "print('Maximum number of atoms: %i' % max_atoms)\n",
        "reshard_size = 256\n",
        "transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)\n",
        "train_dataset.reshard(reshard_size)\n",
        "train_dataset = transformer.transform(train_dataset)\n",
        "test_dataset.reshard(reshard_size)\n",
        "test_dataset = transformer.transform(test_dataset)\n",
        "\n",
        "model = dc.models.DAGModel(\n",
        "            1,\n",
        "            max_atoms=max_atoms,\n",
        "            n_atom_feat=75,\n",
        "            n_graph_feat=30,\n",
        "            n_outputs=30,\n",
        "            batch_size=64,\n",
        "            learning_rate=0.0005,\n",
        "            random_seed=seed,\n",
        "            use_queue=False,\n",
        "            mode='classification')\n",
        "model.fit(train_dataset, nb_epoch= 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJGwzpF4c7Tu"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkfqQl-tbAMc"
      },
      "source": [
        "y_train = train_dataset.y\n",
        "y_train_pred = model.predict(train_dataset)\n",
        "y_t_pred = np.argmax(y_train_pred[:,0], axis=1) \n",
        "y_train_prediction = np.expand_dims(y_t_pred, -1)\n",
        "y_train_prediction = y_train_prediction.astype('float64')\n",
        "\n",
        "y_test = test_dataset.y\n",
        "y_test_pred = model.predict(test_dataset)\n",
        "y_tst_pred = np.argmax(y_test_pred[:,0], axis=1) \n",
        "y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "y_test_prediction = y_test_prediction.astype('float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgivpsOaCvhh"
      },
      "source": [
        "\n",
        "print(\"###################### roc auc score ##############\\n\")\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('Training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('Test set score:', model.evaluate(test_dataset, [metric]))\n",
        "\n",
        "print(\"\\n##################### Recall ####################\")\n",
        "metric = dc.metrics.Metric(dc.metrics.recall_score)\n",
        "print('Training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('Test set score:', model.evaluate(test_dataset, [metric]))\n",
        "\n",
        "print(\"\\n################### precision ################################\")\n",
        "print(\"'Training set score:\", precision_score(y_train, y_train_prediction))\n",
        "print(\"'Test set score:\", precision_score(y_test, y_test_prediction))\n",
        "\n",
        "print(\"\\n################### MCC ################################\")\n",
        "print(\"'Training set score:\", matthews_corrcoef(y_train, y_train_prediction))\n",
        "print(\"'Test set score:\", matthews_corrcoef(y_test, y_test_prediction))\n",
        "\n",
        "print(\"\\n################### cohen's kappa ################################\")\n",
        "print(\"'Training set score:\", cohen_kappa_score(y_train, y_train_prediction))\n",
        "print(\"'Test set score:\", cohen_kappa_score(y_test, y_test_prediction))\n",
        "\n",
        "print(\"\\n################### Accuracy ################################\")\n",
        "print(\"'Training set score:\", accuracy_score(y_train, y_train_prediction))\n",
        "print(\"'Test set score:\", accuracy_score(y_test, y_test_prediction))\n",
        "\n",
        "print(\"################### Balanced accuracy ####################\")\n",
        "print(\"'Training set score:\", balanced_accuracy_score(y_train, y_train_prediction))\n",
        "print(\"'Test set score:\", balanced_accuracy_score(y_test, y_test_prediction))\n",
        "\n",
        "print(\"\\n################### F1 score ####################\\n\")\n",
        "metric = dc.metrics.Metric(dc.metrics.f1_score)\n",
        "print('Training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('Test set score:', model.evaluate(test_dataset, [metric]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}