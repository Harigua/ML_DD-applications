{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepchem_experiments_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uzuhzQteh6Lt",
        "Ntd-JgF6dfAF",
        "PRiOW2Qtdjxt",
        "CnBREfovdZNe",
        "AWAxjyqH0OA6",
        "EVmV0dC60X0H",
        "FdMYqUibCEkz",
        "5_ZO74t1mFtj",
        "ULG1CrvZBwIV",
        "PCaDD2JYwkVL",
        "KJGwzpF4c7Tu",
        "AWY10vJ9Sf3A",
        "YaXcLxFCME__",
        "rTG118GJ9czB",
        "MVt0kHxFLGeg",
        "j9qugXLbLKSa",
        "Avqm1NpoLO2F"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL0RsXoHiK-m"
      },
      "source": [
        "### **Drug Discovery**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzuhzQteh6Lt"
      },
      "source": [
        "## Mount Google drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7oiQ-u_1td6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntd-JgF6dfAF"
      },
      "source": [
        "## Package installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnni4XBKKt2W"
      },
      "source": [
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-py37_4.8.3-Linux-x86_64.sh -b -f -p /usr/local\n",
        "#!time conda install -q -y -c conda-forge rdkit\n",
        "!time conda install -c rdkit rdkit\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB-6AKYKL2wh"
      },
      "source": [
        "!pip install --pre deepchem #\n",
        "!pip install dgl-cu110\n",
        "!pip install dgllife"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGZc3HdLitKz"
      },
      "source": [
        "!pip install xgboost\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRiOW2Qtdjxt"
      },
      "source": [
        "## Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrhw_-Nldos6"
      },
      "source": [
        "import deepchem as dc\n",
        "from deepchem.molnet.preset_hyper_parameters import hps\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnBREfovdZNe"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QwXbN0XZg-v"
      },
      "source": [
        "def load_data(model, data):\n",
        "    \n",
        "    if model in [\"graphconv\", \"dag\"]:\n",
        "      featurizer = dc.feat.ConvMolFeaturizer()\n",
        "    elif model in [\"tf\", \"irv\", \"tf_robust\", \"kernelsvm\", \"rf\", \"logreg\", \"xgb\"]:\n",
        "      featurizer = dc.feat.CircularFingerprint()\n",
        "    elif model in [\"gat\", \"gcn\"]:\n",
        "      featurizer = dc.feat.MolGraphConvFeaturizer()\n",
        "    elif model == \"mpnn\":\n",
        "      featurizer = dc.feat.WeaveFeaturizer()\n",
        "    elif model == \"textcnn\":\n",
        "      featurizer = None\n",
        "\n",
        "    tasks = ['label']\n",
        "    loader = dc.data.CSVLoader(tasks=tasks, feature_field=\"smiles\",featurizer=featurizer)\n",
        "    dataset = loader.create_dataset(data)\n",
        "    transformer = None\n",
        "\n",
        "    return (dataset, [transformer])\n",
        "\n",
        "def load_split(model, train_data, test_data):\n",
        "    \n",
        "    train = None\n",
        "    test  = None\n",
        "    if model in [\"graphconv\", \"dag\"]:\n",
        "      featurizer = dc.feat.ConvMolFeaturizer()\n",
        "    elif model in [\"tf\", \"irv\", \"tf_robust\", \"kernelsvm\", \"rf\", \"logreg\"]:\n",
        "      featurizer = dc.feat.CircularFingerprint()\n",
        "    elif model in [\"gat\", \"gcn\"]:\n",
        "      featurizer = dc.feat.MolGraphConvFeaturizer()\n",
        "    elif model == \"mpnn\":\n",
        "      featurizer = dc.feat.WeaveFeaturizer()\n",
        "    elif model == \"textcnn\":\n",
        "      featurizer = None\n",
        "\n",
        "    \n",
        "    for inputfile in [train_data, test_data]:\n",
        "        \n",
        "        if inputfile != None:\n",
        "            \n",
        "            #print(inputfile)\n",
        "            tasks = ['label']\n",
        "            loader = dc.data.CSVLoader(tasks=tasks, feature_field=\"smiles\",featurizer=featurizer)\n",
        "            dataset = loader.create_dataset(inputfile)\n",
        "            transformer = None\n",
        "            #transformer = dc.trans.BalancingTransformer(dataset=dataset)\n",
        "            #dataset = transformer.transform(dataset)\n",
        "            \n",
        "            if inputfile == train_data:\n",
        "                train = dataset\n",
        "            else:\n",
        "                test = dataset\n",
        "\n",
        "    return (train, test, [transformer])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHobkiRxqdhB"
      },
      "source": [
        "data = \"/mydrive/drug_discovery/heterogene.csv\"\n",
        "dataset, transformers = load_data(\"graphconv\", data)\n",
        "splitter = dc.splits.RandomSplitter()\n",
        "train_dataset, val_dataset, test_dataset = splitter.train_valid_test_split(dataset, frac_train = 0.8, frac_valid = 0.1, frac_test = 0.1, seed=123)\n",
        "#pred_data = \"/mydrive/drug_discovery/gcnsmiles.csv\"\n",
        "#test_dataset, transformers = load_data(\"gcn\", pred_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v97zy3OIZttm",
        "outputId": "5d3c1686-8fa3-479d-c6a9-76dc3087647d"
      },
      "source": [
        "#print(\"Dataset size  : \",train_dataset.X.shape[0] + test_dataset.X.shape[0])\n",
        "print(\"All data size : \",dataset.X.shape[0])\n",
        "print(\"Valset size : \",val_dataset.X.shape[0])\n",
        "print(\"Testset size  : \",test_dataset.X.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All data size :  2601\n",
            "Valset size :  260\n",
            "Testset size  :  261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRwb4DCSdJz0"
      },
      "source": [
        "# Fit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWAxjyqH0OA6"
      },
      "source": [
        "## benchmark classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHrFunSsXdA_"
      },
      "source": [
        "def benchmark_classification(train_dataset,\n",
        "                             tasks,\n",
        "                             transformers,\n",
        "                             n_features,\n",
        "                             metric,\n",
        "                             model,\n",
        "                             valid_dataset= None,\n",
        "                             valid=False,\n",
        "                             test_dataset=None,\n",
        "                             test=False,\n",
        "                             hyper_parameters=None,\n",
        "                             seed=123):\n",
        "  \"\"\"\n",
        "  Calcul la performance de différents modèles sur l'ensemble de données et les tasks spécifiques.\n",
        "  Paramètres\n",
        "  ----------\n",
        "  train_dataset : struct dataset\n",
        "      Jeu de données utilisé pour l'entraînement et l'évaluation du modèle\n",
        "  valid_dataset : struct dataset\n",
        "      jeu de données utilisé uniquement pour l'évaluation du modèle (et le réglage des hyperparamètres)\n",
        "  test_dataset : struct dataset\n",
        "      jeu de données utilisé uniquement pour l'évaluation du modèle\n",
        "  tasks : liste de chaînes de caractères\n",
        "      liste de cibles (tasks, datasets)\n",
        "  transformers : dc.trans.Transformer struct\n",
        "      transformateur utilisé pour l'évaluation du modèle\n",
        "  n_features : integer\n",
        "      nombre de caractéristiques, ou longueur des binary fingerprints\n",
        "  metric : liste d'objets dc.metrics.Metric\n",
        "      métriques utilisées pour l'évaluation\n",
        "  model : chaîne de caractères, facultatif\n",
        "      choix du modèle\n",
        "      rf', 'tf', 'tf_robust', 'logreg', 'irv', 'graphconv', 'dag', 'xgb',\n",
        "      weave', 'kernelsvm', 'textcnn', 'mpnn'.\n",
        "  test : booléen, facultatif\n",
        "      calcul ou non des performances de test_set\n",
        "  hyper_parameters : dict, facultatif (default=None)\n",
        "      paramètres hyper pour le modèle désigné, None = utiliser les valeurs prédéfinies\n",
        "  Retourne\n",
        "  -------\n",
        "  train_scores : dict\n",
        "  résultats de prédiction (AUC) sur l'ensemble d'entraînement\n",
        "  valid_scores : dict\n",
        "  prédiction des résultats (AUC) sur l'ensemble valide\n",
        "  test_scores : dict\n",
        "  prédiction des résultats (AUC) sur l'ensemble de test\n",
        "  \"\"\"\n",
        "  train_scores = {}\n",
        "  valid_scores = {}\n",
        "  test_scores = {}\n",
        "\n",
        "  assert model in [\n",
        "      'rf', 'tf', 'tf_robust', 'logreg', 'irv', 'graphconv', 'dag', 'xgb',\n",
        "      'weave', 'kernelsvm', 'textcnn', 'mpnn', 'gat', 'gcn'\n",
        "  ]\n",
        "  if hyper_parameters is None and model not in ['gat', 'gcn']:\n",
        "    hyper_parameters = hps[model]\n",
        "  model_name = model\n",
        "\n",
        "  if model_name == \"gat\":\n",
        "    nb_epoch = 40\n",
        "    model = dc.models.GATModel(1,\n",
        "                 mode='classification',\n",
        "                 batch_size=32,\n",
        "                 learning_rate=0.0001,\n",
        "                 dropout=0.25,\n",
        "                 )\n",
        "  elif model_name == \"gcn\":\n",
        "    nb_epoch = 40\n",
        "    model = dc.models.GCNModel(1,\n",
        "                 mode='classification',\n",
        "                 batch_size=32,\n",
        "                 learning_rate=0.001,\n",
        "                 dropout=0.1,\n",
        "                 )\n",
        "\n",
        "  elif model_name == 'tf':\n",
        "    layer_sizes = hyper_parameters['layer_sizes']\n",
        "    weight_init_stddevs = hyper_parameters['weight_init_stddevs']\n",
        "    bias_init_consts = hyper_parameters['bias_init_consts']\n",
        "    dropouts = hyper_parameters['dropouts']\n",
        "    penalty = hyper_parameters['penalty']\n",
        "    penalty_type = hyper_parameters['penalty_type']\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "\n",
        "    # Building tensorflow MultitaskDNN model\n",
        "    model = dc.models.MultitaskClassifier(\n",
        "        len(tasks),\n",
        "        2048,\n",
        "        layer_sizes=layer_sizes,\n",
        "        weight_init_stddevs=weight_init_stddevs,\n",
        "        bias_init_consts=bias_init_consts,\n",
        "        dropouts=dropouts,\n",
        "        weight_decay_penalty=penalty,\n",
        "        weight_decay_penalty_type=penalty_type,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed)\n",
        "\n",
        "  elif model_name == 'tf_robust':\n",
        "    layer_sizes = hyper_parameters['layer_sizes']\n",
        "    weight_init_stddevs = hyper_parameters['weight_init_stddevs']\n",
        "    bias_init_consts = hyper_parameters['bias_init_consts']\n",
        "    dropouts = hyper_parameters['dropouts']\n",
        "\n",
        "    bypass_layer_sizes = hyper_parameters['bypass_layer_sizes']\n",
        "    bypass_weight_init_stddevs = hyper_parameters['bypass_weight_init_stddevs']\n",
        "    bypass_bias_init_consts = hyper_parameters['bypass_bias_init_consts']\n",
        "    bypass_dropouts = hyper_parameters['bypass_dropouts']\n",
        "\n",
        "    penalty = hyper_parameters['penalty']\n",
        "    penalty_type = hyper_parameters['penalty_type']\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "\n",
        "    # Building tensorflow robust MultitaskDNN model\n",
        "    model = dc.models.RobustMultitaskClassifier(\n",
        "        len(tasks),\n",
        "        n_features,\n",
        "        layer_sizes=layer_sizes,\n",
        "        weight_init_stddevs=weight_init_stddevs,\n",
        "        bias_init_consts=bias_init_consts,\n",
        "        dropouts=dropouts,\n",
        "        bypass_layer_sizes=bypass_layer_sizes,\n",
        "        bypass_weight_init_stddevs=bypass_weight_init_stddevs,\n",
        "        bypass_bias_init_consts=bypass_bias_init_consts,\n",
        "        bypass_dropouts=bypass_dropouts,\n",
        "        weight_decay_penalty=penalty,\n",
        "        weight_decay_penalty_type=penalty_type,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed)\n",
        "\n",
        "  elif model_name == 'logreg':\n",
        "    penalty = hyper_parameters['penalty']\n",
        "    penalty_type = hyper_parameters['penalty_type']\n",
        "    nb_epoch = None\n",
        "\n",
        "    # Building scikit logistic regression model\n",
        "    def model_builder(model_dir):\n",
        "      sklearn_model = LogisticRegression(\n",
        "          penalty=penalty_type,\n",
        "          C=1. / penalty,\n",
        "          class_weight=\"balanced\",\n",
        "          n_jobs=-1)\n",
        "      return dc.models.sklearn_models.SklearnModel(\n",
        "          sklearn_model, model_dir)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        tasks, model_builder)\n",
        "\n",
        "  elif model_name == 'irv':\n",
        "    penalty = hyper_parameters['penalty']\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_K = hyper_parameters['n_K']\n",
        "\n",
        "    # Transform fingerprints to IRV features\n",
        "    transformer = dc.trans.IRVTransformer(n_K, len(tasks), train_dataset)\n",
        "    train_dataset = transformer.transform(train_dataset)\n",
        "    if valid:\n",
        "      valid_dataset = transformer.transform(valid_dataset)\n",
        "    if test:\n",
        "      test_dataset = transformer.transform(test_dataset)\n",
        "\n",
        "    # Building tensorflow IRV model\n",
        "    model = dc.models.MultitaskIRVClassifier(\n",
        "        len(tasks),\n",
        "        K=n_K,\n",
        "        penalty=penalty,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'graphconv':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_filters = hyper_parameters['n_filters']\n",
        "    n_fully_connected_nodes = hyper_parameters['n_fully_connected_nodes']\n",
        "\n",
        "    model = dc.models.GraphConvModel(\n",
        "        len(tasks),\n",
        "        graph_conv_layers=[n_filters] * 2,\n",
        "        dense_layer_size=n_fully_connected_nodes,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'dag':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_graph_feat = hyper_parameters['n_graph_feat']\n",
        "    default_max_atoms = hyper_parameters['default_max_atoms']\n",
        "\n",
        "    max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])\n",
        "    if valid and test:\n",
        "      max_atoms = max([max_atoms_train, max_atoms_valid, max_atoms_test])\n",
        "    elif valid:\n",
        "      max_atoms_valid = max([mol.get_num_atoms() for mol in valid_dataset.X])\n",
        "      max_atoms = max([max_atoms_train, max_atoms_valid])\n",
        "    elif test :\n",
        "      max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])\n",
        "      max_atoms = max([max_atoms_train, max_atoms_test])\n",
        "    else:\n",
        "      max_atoms = max([max_atoms_train])\n",
        "\n",
        "    max_atoms = min([max_atoms, default_max_atoms])\n",
        "    print('Maximum number of atoms: %i' % max_atoms)\n",
        "    reshard_size = 256\n",
        "    transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)\n",
        "    train_dataset.reshard(reshard_size)\n",
        "    train_dataset = transformer.transform(train_dataset)\n",
        "    if valid:\n",
        "      valid_dataset.reshard(reshard_size)\n",
        "      valid_dataset = transformer.transform(valid_dataset)\n",
        "    if test:\n",
        "      test_dataset.reshard(reshard_size)\n",
        "      test_dataset = transformer.transform(test_dataset)\n",
        "\n",
        "    model = dc.models.DAGModel(\n",
        "        len(tasks),\n",
        "        max_atoms=max_atoms,\n",
        "        n_atom_feat=75,\n",
        "        n_graph_feat=n_graph_feat,\n",
        "        n_outputs=30,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        random_seed=seed,\n",
        "        use_queue=False,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'weave':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_graph_feat = hyper_parameters['n_graph_feat']\n",
        "    n_pair_feat = hyper_parameters['n_pair_feat']\n",
        "\n",
        "    model = dc.models.WeaveModel(\n",
        "        len(tasks),\n",
        "        n_atom_feat=n_features,\n",
        "        n_pair_feat=n_pair_feat,\n",
        "        n_hidden=50,\n",
        "        n_graph_feat=n_graph_feat,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        use_queue=False,\n",
        "        random_seed=seed,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'textcnn':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_embedding = hyper_parameters['n_embedding']\n",
        "    filter_sizes = hyper_parameters['filter_sizes']\n",
        "    num_filters = hyper_parameters['num_filters']\n",
        "\n",
        "    all_data = dc.data.DiskDataset.merge(\n",
        "        [train_dataset, valid_dataset, test_dataset])\n",
        "    char_dict, length = dc.models.TextCNNModel.build_char_dict(all_data)\n",
        "\n",
        "    model = dc.models.TextCNNModel(\n",
        "        len(tasks),\n",
        "        char_dict,\n",
        "        seq_length=length,\n",
        "        n_embedding=n_embedding,\n",
        "        filter_sizes=filter_sizes,\n",
        "        num_filters=num_filters,\n",
        "        learning_rate=learning_rate,\n",
        "        batch_size=batch_size,\n",
        "        use_queue=False,\n",
        "        random_seed=seed,\n",
        "        mode='classification')\n",
        "\n",
        "  elif model_name == 'mpnn':\n",
        "    batch_size = hyper_parameters['batch_size']\n",
        "    nb_epoch = hyper_parameters['nb_epoch']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    T = hyper_parameters['T']\n",
        "    M = hyper_parameters['M']\n",
        "\n",
        "    model = dc.models.MPNNModel(\n",
        "        len(tasks),\n",
        "        n_atom_feat=n_features[0],\n",
        "        n_pair_feat=n_features[1],\n",
        "        n_hidden=n_features[0],\n",
        "        T=T,\n",
        "        M=M,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        use_queue=False,\n",
        "        mode=\"classification\")\n",
        "\n",
        "  elif model_name == 'rf':\n",
        "    n_estimators = hyper_parameters['n_estimators']\n",
        "    nb_epoch = None\n",
        "\n",
        "    # Building scikit random forest model\n",
        "    def model_builder(model_dir):\n",
        "      sklearn_model = RandomForestClassifier(\n",
        "          class_weight=\"balanced\", n_estimators=n_estimators, n_jobs=-1)\n",
        "      return dc.models.sklearn_models.SklearnModel(\n",
        "          sklearn_model, model_dir)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        tasks, model_builder)\n",
        "\n",
        "  elif model_name == 'kernelsvm':\n",
        "    C = hyper_parameters['C']\n",
        "    gamma = hyper_parameters['gamma']\n",
        "    nb_epoch = None\n",
        "\n",
        "    # Building scikit learn Kernel SVM model\n",
        "    def model_builder(model_dir):\n",
        "      sklearn_model = SVC(\n",
        "          C=C, gamma=gamma, class_weight=\"balanced\", probability=True)\n",
        "      return dc.models.SklearnModel(sklearn_model, model_dir)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        tasks, model_builder)\n",
        "\n",
        "  elif model_name == 'xgb':\n",
        "    max_depth = hyper_parameters['max_depth']\n",
        "    learning_rate = hyper_parameters['learning_rate']\n",
        "    n_estimators = hyper_parameters['n_estimators']\n",
        "    gamma = hyper_parameters['gamma']\n",
        "    min_child_weight = hyper_parameters['min_child_weight']\n",
        "    max_delta_step = hyper_parameters['max_delta_step']\n",
        "    subsample = hyper_parameters['subsample']\n",
        "    colsample_bytree = hyper_parameters['colsample_bytree']\n",
        "    colsample_bylevel = hyper_parameters['colsample_bylevel']\n",
        "    reg_alpha = hyper_parameters['reg_alpha']\n",
        "    reg_lambda = hyper_parameters['reg_lambda']\n",
        "    scale_pos_weight = hyper_parameters['scale_pos_weight']\n",
        "    base_score = hyper_parameters['base_score']\n",
        "    seed = hyper_parameters['seed']\n",
        "    early_stopping_rounds = hyper_parameters['early_stopping_rounds']\n",
        "    nb_epoch = None\n",
        "\n",
        "    esr = {'early_stopping_rounds': early_stopping_rounds}\n",
        "\n",
        "    # Building xgboost classification model\n",
        "    def model_builder(model_dir):\n",
        "      import xgboost\n",
        "      xgboost_model = xgboost.XGBClassifier(\n",
        "          max_depth=max_depth,\n",
        "          learning_rate=learning_rate,\n",
        "          n_estimators=n_estimators,\n",
        "          gamma=gamma,\n",
        "          min_child_weight=min_child_weight,\n",
        "          max_delta_step=max_delta_step,\n",
        "          subsample=subsample,\n",
        "          colsample_bytree=colsample_bytree,\n",
        "          colsample_bylevel=colsample_bylevel,\n",
        "          reg_alpha=reg_alpha,\n",
        "          reg_lambda=reg_lambda,\n",
        "          scale_pos_weight=scale_pos_weight,\n",
        "          base_score=base_score,\n",
        "          seed=seed)\n",
        "      return dc.models.GBDTModel(\n",
        "          xgboost_model, model_dir, **esr)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        tasks, model_builder)\n",
        "\n",
        "  if nb_epoch is None:\n",
        "    model.fit(train_dataset)\n",
        "  else:\n",
        "    model.fit(train_dataset, nb_epoch=nb_epoch)\n",
        "\n",
        "  train_scores[model_name] = model.evaluate(train_dataset, metric)\n",
        "  \n",
        "  if valid:\n",
        "    valid_scores[model_name] = model.evaluate(valid_dataset, metric)\n",
        "  if test:\n",
        "    test_scores[model_name] = model.evaluate(test_dataset, metric)\n",
        "\n",
        "  return train_scores, valid_scores, test_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVmV0dC60X0H"
      },
      "source": [
        "## Instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6lNpxmmveFX"
      },
      "source": [
        "data = \"/mydrive/Test/viral_cov-2_train.csv\"\n",
        "datasetconvmol, transformers = load_data(\"graphconv\", data)\n",
        "datasetcircl, transformers = load_data(\"tf\", data)\n",
        "datasetmolgraph, transformers = load_data(\"gat\", data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoYeE8XXcXyn"
      },
      "source": [
        "#random split\n",
        "splitter = dc.splits.RandomSplitter()\n",
        "#scaffold split\n",
        "#splitter = dc.splits.ScaffoldSplitter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NbXgswbgAG4"
      },
      "source": [
        "metric = [dc.metrics.Metric(dc.metrics.roc_auc_score), dc.metrics.Metric(dc.metrics.f1_score), dc.metrics.Metric(dc.metrics.recall_score)]\n",
        "\n",
        "train_roc = []\n",
        "#val_roc = []\n",
        "#test_roc = []\n",
        "\n",
        "train_F1 = []\n",
        "#val_F1 = []\n",
        "#test_F1 = []\n",
        "\n",
        "train_recall = []\n",
        "#val_recall = []\n",
        "#test_recall = []\n",
        "\n",
        "import pandas as pd\n",
        "metric_results = pd.DataFrame()\n",
        "\n",
        "mesmodeles = [\"graphconv\", \"dag\", \"gat\", \"gcn\", \"logreg\", \"kernelsvm\", \"rf\", \"tf\", \"irv\", \"tf_robust\", \"xgb\"]\n",
        "frct = 1.0\n",
        "frctvl= 0.0\n",
        "\n",
        "for mdl in mesmodeles :\n",
        "  print(mdl)\n",
        "  if mdl in [\"graphconv\", \"dag\"]:\n",
        "    train_dataset, val_dataset, test_dataset = splitter.train_valid_test_split(datasetconvmol, frac_train = frct, frac_valid = frctvl, frac_test = frctvl, seed=123)\n",
        "    \n",
        "  elif mdl in [\"tf\", \"irv\", \"tf_robust\", \"kernelsvm\", \"rf\", \"logreg\", \"xgb\"]:\n",
        "    train_dataset, val_dataset, test_dataset = splitter.train_valid_test_split(datasetcircl, frac_train = frct, frac_valid = frctvl, frac_test = frctvl, seed=123)\n",
        "  else:\n",
        "    train_dataset, val_dataset, test_dataset = splitter.train_valid_test_split(datasetmolgraph, frac_train = frct, frac_valid = frctvl, frac_test = frctvl, seed=123)\n",
        "\n",
        "  train_scores, _valid_scores, _test_scores  = benchmark_classification(train_dataset = train_dataset,\n",
        "                                                                      tasks = [1],\n",
        "                                                                      transformers = transformers,\n",
        "                                                                      n_features = 2048,\n",
        "                                                                      metric = metric,\n",
        "                                                                      model = mdl,\n",
        "                                                                      hyper_parameters=None,\n",
        "                                                                      seed=123)\n",
        "  train_roc.append(round(train_scores[mdl]['roc_auc_score'],2))\n",
        "  #val_roc.append(round(valid_scores[mdl]['roc_auc_score'],2))\n",
        "  #test_roc.append(round(test_scores[mdl]['roc_auc_score'],2))\n",
        "\n",
        "  train_F1.append(round(train_scores[mdl]['f1_score'],2))\n",
        "  #val_F1.append(round(valid_scores[mdl]['f1_score'],2))\n",
        "  #test_F1.append(round(test_scores[mdl]['f1_score'],2))\n",
        "\n",
        "  train_recall.append(round(train_scores[mdl]['recall_score'],2))\n",
        "  #val_recall.append(round(valid_scores[mdl]['recall_score'],2))\n",
        "  #test_recall.append(round(test_scores[mdl]['recall_score'],2))\n",
        "\n",
        "  metric_results[\"Train ROC-AUC\"] = train_roc\n",
        "  #metric_results[\"Val ROC-AUC\"] = val_roc\n",
        "  #metric_results[\"Test ROC-AUC\"] = test_roc\n",
        "\n",
        "  metric_results[\"Train F1 score\"] = train_F1\n",
        "  #metric_results[\"Val F1 score\"] = val_F1\n",
        "  #metric_results[\"Test F1 score\"] = test_F1\n",
        "\n",
        "  metric_results[\"Train Recall\"] = train_recall\n",
        "  #metric_results[\"Val Recall\"] = val_recall\n",
        "  #metric_results[\"Test Recall\"] = test_recall\n",
        "\n",
        "  metric_results.to_csv(\"/mydrive/Test/scores/viral_cov-2_train_score.csv\", index=False)\n",
        "  metric_results = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdMYqUibCEkz"
      },
      "source": [
        "#Hyperparameter Tuning\n",
        "One of the most important aspects of machine learning is hyperparameter tuning. Many machine learning models have a number of hyperparameters that control aspects of the model. These hyperparameters typically cannot be learned directly by the same learning algorithm used for the rest of learning and have to be set in an alternate fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_ZO74t1mFtj"
      },
      "source": [
        "###GCNModel model Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUNPO-h9mLsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f287b151-fa86-4295-dd8f-35d1218eddac"
      },
      "source": [
        "nb_epoch = [120, 40]\n",
        "batch_size = [64, 32, 16]\n",
        "learning_rate = [0.001, 0.0001]\n",
        "dropout = [0.15, 0.10]\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "\n",
        "param_nb_epoch = []\n",
        "param_batch_size = []\n",
        "param_learning_rate =[]\n",
        "param_dropout =[]\n",
        "test_acc = []\n",
        "test_auc = []\n",
        "import pandas as pd\n",
        "parameters_results = pd.DataFrame()\n",
        "\n",
        "for ep in nb_epoch:\n",
        "  for bsiz in batch_size:\n",
        "    for lgr in learning_rate:\n",
        "      for dp in dropout:\n",
        "        print(\"Curent params : \", ep, bsiz, lgr, dp)\n",
        "        print(\"Instanciate model...\\n\")\n",
        "        model = dc.models.GCNModel(1,\n",
        "                 mode='classification',\n",
        "                 batch_size=bsiz,\n",
        "                 learning_rate=lgr,\n",
        "                 dropout=dp,\n",
        "                 )\n",
        "        print(\"Fitting model...\")\n",
        "        model.fit(train_dataset, nb_epoch= ep)\n",
        "        test_score = model.evaluate(test_dataset, [metric])\n",
        "        y_test = test_dataset.y\n",
        "        y_test_pred = model.predict(test_dataset)\n",
        "        y_tst_pred = np.argmax(y_test_pred, axis=1) \n",
        "        y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "        y_test_prediction = y_test_prediction.astype('float64')\n",
        "\n",
        "        param_nb_epoch.append(ep)\n",
        "        param_batch_size.append(bsiz)\n",
        "        param_learning_rate.append(lgr)\n",
        "        param_dropout.append(dp)\n",
        "        test_acc.append(round(accuracy_score(y_test, y_test_prediction), 2))\n",
        "        test_auc.append(round(test_score[\"roc_auc_score\"], 2))\n",
        "\n",
        "  parameters_results[\"epoch\"] = param_nb_epoch\n",
        "  parameters_results[\"dropout\"] = param_dropout\n",
        "  parameters_results[\"learning_rate\"] = param_learning_rate\n",
        "  parameters_results[\"batch_size\"] = param_batch_size\n",
        "  parameters_results[\"accuracy\"] = test_acc\n",
        "  parameters_results[\"auc roc score\"] = test_auc\n",
        "  parameters_results.to_csv(\"/mydrive/drug_discovery/GCNModel_Opt.csv\", index=False)\n",
        "  parameters_results = pd.DataFrame()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Curent params :  120 64 0.001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 64 0.001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 64 0.0001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 64 0.0001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 32 0.001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 32 0.001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 32 0.0001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 32 0.0001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 16 0.001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 16 0.001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 16 0.0001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  120 16 0.0001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 64 0.001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 64 0.001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 64 0.0001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 64 0.0001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 32 0.001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 32 0.001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 32 0.0001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 32 0.0001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 16 0.001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 16 0.001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 16 0.0001 0.15\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n",
            "Curent params :  40 16 0.0001 0.1\n",
            "Instanciate model...\n",
            "\n",
            "Fitting model...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULG1CrvZBwIV"
      },
      "source": [
        "### RF Model Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF1UwUahBuAg"
      },
      "source": [
        "model = RandomForestClassifier()\n",
        "n_estimators = [10, 100, 300, 500, 600, 1000]\n",
        "max_features = ['sqrt', 'log2']\n",
        "nb_epoch = None\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "\n",
        "import pandas as pd\n",
        "parameters_results = pd.DataFrame()\n",
        "param_n_estimators = []\n",
        "param_max_features = []\n",
        "test_acc = []\n",
        "test_auc = []\n",
        "\n",
        "for n_est in n_estimators:\n",
        "  for max_f in max_features:\n",
        "    print(\"Curent params : \", n_est, max_f)\n",
        "    print(\"Instanciate model...\")\n",
        "    # Building scikit random forest model\n",
        "    def model_builder(model_dir):\n",
        "      sklearn_model = RandomForestClassifier(\n",
        "          class_weight = \"balanced\", n_estimators = n_est, max_features = max_f, n_jobs=-1)\n",
        "      return dc.models.sklearn_models.SklearnModel(\n",
        "          sklearn_model, model_dir)\n",
        "\n",
        "    model = dc.models.multitask.SingletaskToMultitask(\n",
        "        [1], model_builder)\n",
        "    print(\"Fitting model...\\n\")\n",
        "    model.fit(train_dataset)\n",
        "\n",
        "    test_score = model.evaluate(test_dataset, [metric])\n",
        "    y_test = test_dataset.y\n",
        "    y_test_pred = model.predict(test_dataset)\n",
        "    y_tst_pred = np.argmax(y_test_pred[:,0], axis=1) \n",
        "    y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "    y_test_prediction = y_test_prediction.astype('float64')\n",
        "\n",
        "    param_n_estimators.append(n_est)\n",
        "    param_max_features.append(max_f)\n",
        "    test_acc.append(round(accuracy_score(y_test, y_test_prediction), 2))\n",
        "    test_auc.append(round(test_score[\"roc_auc_score\"], 2))\n",
        "\n",
        "  parameters_results[\"n_estimators\"] = param_n_estimators\n",
        "  parameters_results[\"max_features\"] = param_max_features\n",
        "  parameters_results[\"accuracy\"] = test_acc\n",
        "  parameters_results[\"auc roc score\"] = test_auc\n",
        "  parameters_results.to_csv(\"/mydrive/drug_discovery/RF_Opt.csv\", index=False)\n",
        "  parameters_results = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCaDD2JYwkVL"
      },
      "source": [
        "### DAGModel Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8CGBPxDwvjp"
      },
      "source": [
        "batch_size = [64, 32]  #hyper_parameters['batch_size']\n",
        "nb_epoch = [50, 40] #hyper_parameters['nb_epoch']\n",
        "learning_rate = [0.0005, 0.001] #hyper_parameters['learning_rate']\n",
        "n_graph_feat =  [30, 20] #hyper_parameters['n_graph_feat']\n",
        "default_max_atoms = 60 #hyper_parameters['default_max_atoms']\n",
        "seed = 123\n",
        "max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])\n",
        "max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])\n",
        "max_atoms = max([max_atoms_train, max_atoms_test])\n",
        "max_atoms = min([max_atoms, default_max_atoms])\n",
        "print('Maximum number of atoms: %i' % max_atoms)\n",
        "reshard_size = 256\n",
        "transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)\n",
        "train_dataset.reshard(reshard_size)\n",
        "train_dataset = transformer.transform(train_dataset)\n",
        "test_dataset.reshard(reshard_size)\n",
        "test_dataset = transformer.transform(test_dataset)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "\n",
        "parambatch_size = []\n",
        "paramnb_epoch = []\n",
        "paramlearning_rate = []\n",
        "paramn_graph_feat =  []\n",
        "test_acc = []\n",
        "test_auc = []\n",
        "import pandas as pd\n",
        "parameters_results = pd.DataFrame()\n",
        "for btch_s in batch_size:\n",
        "  for nb_ep in nb_epoch:\n",
        "    for lrg_r in learning_rate:\n",
        "      for n_gph in n_graph_feat:\n",
        "        print(\"current conf : \", nb_ep, btch_s, lrg_r, n_gph)\n",
        "        print(\"instantiate the model...\")\n",
        "        model = dc.models.DAGModel(\n",
        "            1,\n",
        "            max_atoms=max_atoms,\n",
        "            n_atom_feat=75,\n",
        "            n_graph_feat=n_gph,\n",
        "            n_outputs=30,\n",
        "            batch_size=btch_s,\n",
        "            learning_rate=lrg_r,\n",
        "            random_seed=seed,\n",
        "            use_queue=False,\n",
        "            mode='classification')\n",
        "        \n",
        "        print(\"Fitting the model...\")\n",
        "        model.fit(train_dataset, nb_epoch= nb_ep)\n",
        "        print(\"evaluate the model...\")\n",
        "        test_score = model.evaluate(test_dataset, [metric])\n",
        "        y_test = test_dataset.y\n",
        "        y_test_pred = model.predict(test_dataset)\n",
        "        y_tst_pred = np.argmax(y_test_pred[:,0], axis=1) \n",
        "        y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "        y_test_prediction = y_test_prediction.astype('float64')\n",
        "\n",
        "        paramnb_epoch.append(nb_ep)\n",
        "        parambatch_size.append(btch_s)\n",
        "        paramlearning_rate.append(lrg_r)\n",
        "        paramn_graph_feat.append(n_gph)\n",
        "        test_acc.append(round(accuracy_score(y_test, y_test_prediction), 2))\n",
        "        test_auc.append(round(test_score[\"roc_auc_score\"], 2))\n",
        "  print(\"save the model...\\n\")\n",
        "  parameters_results[\"epoch\"] = paramnb_epoch\n",
        "  parameters_results[\"n_graph_feat\"] = paramn_graph_feat\n",
        "  parameters_results[\"learning_rate\"] = paramlearning_rate\n",
        "  parameters_results[\"batch_size\"] = parambatch_size\n",
        "  parameters_results[\"accuracy\"] = test_acc\n",
        "  parameters_results[\"auc roc score\"] = test_auc\n",
        "  parameters_results.to_csv(\"/mydrive/drug_discovery/DAGModel_Opt.csv\", index=False)\n",
        "  parameters_results = pd.DataFrame()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJGwzpF4c7Tu"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyly7_fUaqbc"
      },
      "source": [
        "print(\"###################### roc auc score ##############\\n\")\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('Training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('Test set score:', model.evaluate(test_dataset, [metric]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrsqdt4tV-iP"
      },
      "source": [
        "print(\"\\n################### F1 score ####################\\n\")\n",
        "metric = dc.metrics.Metric(dc.metrics.f1_score)\n",
        "print('Training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('Test set score:', model.evaluate(test_dataset, [metric]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDe2Ve8pce88"
      },
      "source": [
        "print(\"\\n##################### Recall ####################\")\n",
        "metric = dc.metrics.Metric(dc.metrics.recall_score)\n",
        "print('Training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('Test set score:', model.evaluate(test_dataset, [metric]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgivpsOaCvhh"
      },
      "source": [
        "y_train = train_dataset.y\n",
        "y_train_pred = model.predict(train_dataset)\n",
        "y_t_pred = np.argmax(y_train_pred[:,0], axis=1) \n",
        "y_train_prediction = np.expand_dims(y_t_pred, -1)\n",
        "#print(y_train_prediction.shape)\n",
        "#print(y_train_prediction.dtype)\n",
        "y_train_prediction = y_train_prediction.astype('float64')\n",
        "\n",
        "y_test = test_dataset.y\n",
        "y_test_pred = model.predict(test_dataset)\n",
        "y_tst_pred = np.argmax(y_test_pred[:,0], axis=1) \n",
        "y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "y_test_prediction = y_test_prediction.astype('float64')\n",
        "\n",
        "print(\"################### Balanced accuracy ####################\")\n",
        "print(\"'Training set score:\", balanced_accuracy_score(y_train, y_train_prediction))\n",
        "print(\"'Test set score:\", balanced_accuracy_score(y_test, y_test_prediction))\n",
        "\n",
        "print(\"\\n################### MCC ################################\")\n",
        "print(\"'Training set score:\", matthews_corrcoef(y_train, y_train_prediction))\n",
        "print(\"'Test set score:\", matthews_corrcoef(y_test, y_test_prediction))\n",
        "\n",
        "print(\"\\n################### Accuracy ################################\")\n",
        "print(\"'Training set score:\", accuracy_score(y_train, y_train_prediction))\n",
        "print(\"'Test set score:\", accuracy_score(y_test, y_test_prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWY10vJ9Sf3A"
      },
      "source": [
        "#10-cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg4Kd-MvWZLn"
      },
      "source": [
        "train_roc = []\n",
        "val_roc = []\n",
        "\n",
        "train_F1 = []\n",
        "val_F1 = []\n",
        "\n",
        "train_recall = []\n",
        "val_recall = []\n",
        "\n",
        "kf = []\n",
        "n = 1\n",
        "data = \"/mydrive/drug_discovery/data.csv\"\n",
        "import pandas as pd\n",
        "metric_results = pd.DataFrame()\n",
        "metric = [dc.metrics.Metric(dc.metrics.roc_auc_score), dc.metrics.Metric(dc.metrics.f1_score), dc.metrics.Metric(dc.metrics.recall_score)]\n",
        "splitter = dc.splits.RandomSplitter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCyP3X79Tk7H"
      },
      "source": [
        "RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ax9zE0sSfcn",
        "outputId": "da593721-bd3c-43b9-a124-57893e59e74d"
      },
      "source": [
        "dataset, transformers = load_data(\"tf\", data)\n",
        "split_datas = splitter.k_fold_split(dataset,10)\n",
        "\n",
        "def model_builder(model_dir):\n",
        "\n",
        "  sklearn_model = RandomForestClassifier(\n",
        "      class_weight = \"balanced\", n_estimators = 300, max_features = 'sqrt', n_jobs=-1)\n",
        "  return dc.models.sklearn_models.SklearnModel(\n",
        "      sklearn_model, model_dir)\n",
        "\n",
        "for train_set, val_set in split_datas:\n",
        "\n",
        "  print(\"Fold : \", n)\n",
        "\n",
        "  model = dc.models.multitask.SingletaskToMultitask([1], model_builder)\n",
        "  model.fit(train_set)\n",
        "\n",
        "  train_scores = model.evaluate(train_set, metric)\n",
        "  valid_scores = model.evaluate(val_set, metric)\n",
        "\n",
        "  kf.append(n)\n",
        "  n += 1\n",
        "\n",
        "  train_roc.append(round(train_scores['roc_auc_score'],2))\n",
        "  val_roc.append(round(valid_scores['roc_auc_score'],2))\n",
        "\n",
        "  train_F1.append(round(train_scores['f1_score'],2))\n",
        "  val_F1.append(round(valid_scores['f1_score'],2))\n",
        "\n",
        "  train_recall.append(round(train_scores['recall_score'],2))\n",
        "  val_recall.append(round(valid_scores['recall_score'],2))\n",
        "\n",
        "metric_results[\"K\"] = kf \n",
        "metric_results[\"Train ROC-AUC\"] = train_roc\n",
        "metric_results[\"Val ROC-AUC\"] = val_roc\n",
        "\n",
        "metric_results[\"Train F1 score\"] = train_F1\n",
        "metric_results[\"Val F1 score\"] = val_F1\n",
        "\n",
        "metric_results[\"Train Recall\"] = train_recall\n",
        "metric_results[\"Val Recall\"] = val_recall\n",
        "\n",
        "metric_results.to_csv(\"/mydrive/cross_val/cross_val_rf.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold :  1\n",
            "Fold :  2\n",
            "Fold :  3\n",
            "Fold :  4\n",
            "Fold :  5\n",
            "Fold :  6\n",
            "Fold :  7\n",
            "Fold :  8\n",
            "Fold :  9\n",
            "Fold :  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eiZXqB1Tn6g"
      },
      "source": [
        "GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDU8qi9gTSdv",
        "outputId": "dfaa55e2-24f6-4a95-8273-2fed56217d81"
      },
      "source": [
        "dataset, transformers = load_data(\"gat\", data)\n",
        "split_datas = splitter.k_fold_split(dataset,10)\n",
        "\n",
        "for train_set, val_set in split_datas:\n",
        "\n",
        "  print(\"Fold : \", n)\n",
        "  model = dc.models.GCNModel(1,\n",
        "                mode='classification',\n",
        "                batch_size=64,\n",
        "                learning_rate=0.001,\n",
        "                dropout=0.1,\n",
        "                )\n",
        "  model.fit(train_set, nb_epoch= 40)\n",
        "\n",
        "  train_scores = model.evaluate(train_set, metric)\n",
        "  valid_scores = model.evaluate(val_set, metric)\n",
        "\n",
        "  kf.append(n)\n",
        "  n += 1\n",
        "\n",
        "  train_roc.append(round(train_scores['roc_auc_score'],2))\n",
        "  val_roc.append(round(valid_scores['roc_auc_score'],2))\n",
        "\n",
        "  train_F1.append(round(train_scores['f1_score'],2))\n",
        "  val_F1.append(round(valid_scores['f1_score'],2))\n",
        "\n",
        "  train_recall.append(round(train_scores['recall_score'],2))\n",
        "  val_recall.append(round(valid_scores['recall_score'],2))\n",
        "\n",
        "metric_results[\"K\"] = kf \n",
        "metric_results[\"Train ROC-AUC\"] = train_roc\n",
        "metric_results[\"Val ROC-AUC\"] = val_roc\n",
        "\n",
        "metric_results[\"Train F1 score\"] = train_F1\n",
        "metric_results[\"Val F1 score\"] = val_F1\n",
        "\n",
        "metric_results[\"Train Recall\"] = train_recall\n",
        "metric_results[\"Val Recall\"] = val_recall\n",
        "\n",
        "metric_results.to_csv(\"/mydrive/cross_val/cross_val_gcn.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold :  1\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold :  2\n",
            "Fold :  3\n",
            "Fold :  4\n",
            "Fold :  5\n",
            "Fold :  6\n",
            "Fold :  7\n",
            "Fold :  8\n",
            "Fold :  9\n",
            "Fold :  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr55CLv1Tp2H"
      },
      "source": [
        "DAG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz74ffwETS0X"
      },
      "source": [
        "dataset, transformers = load_data(\"graphconv\", data)\n",
        "split_datas = splitter.k_fold_split(dataset,10, seed= 123)\n",
        "\n",
        "for i , (train_set, val_set) in enumerate(split_datas):\n",
        "  print(i)\n",
        "  if i in [3, 4, 5]:\n",
        "\n",
        "    print(\"Fold : \", i+1)\n",
        "    default_max_atoms = 60 \n",
        "    seed = 123\n",
        "    max_atoms_train = max([mol.get_num_atoms() for mol in train_set.X])\n",
        "    max_atoms_test = max([mol.get_num_atoms() for mol in val_set.X])\n",
        "    max_atoms = max([max_atoms_train, max_atoms_test])\n",
        "    max_atoms = min([max_atoms, default_max_atoms])\n",
        "    print('Maximum number of atoms: %i' % max_atoms)\n",
        "    reshard_size = 256\n",
        "    transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)\n",
        "    train_set.reshard(reshard_size)\n",
        "    train_set = transformer.transform(train_set)\n",
        "    val_set.reshard(reshard_size)\n",
        "    val_set = transformer.transform(val_set)\n",
        "\n",
        "    model = dc.models.DAGModel(\n",
        "                1,\n",
        "                max_atoms=max_atoms,\n",
        "                n_atom_feat=75,\n",
        "                n_graph_feat=30,\n",
        "                n_outputs=30,\n",
        "                batch_size=64,\n",
        "                learning_rate=0.0005,\n",
        "                random_seed=seed,\n",
        "                use_queue=False,\n",
        "                mode='classification')\n",
        "    model.fit(train_set, nb_epoch= 40)\n",
        "\n",
        "    train_scores = model.evaluate(train_set, metric)\n",
        "    valid_scores = model.evaluate(val_set, metric)\n",
        "\n",
        "    kf.append(i+1)\n",
        "    n += 1\n",
        "\n",
        "    train_roc.append(round(train_scores['roc_auc_score'],2))\n",
        "    val_roc.append(round(valid_scores['roc_auc_score'],2))\n",
        "\n",
        "    train_F1.append(round(train_scores['f1_score'],2))\n",
        "    val_F1.append(round(valid_scores['f1_score'],2))\n",
        "\n",
        "    train_recall.append(round(train_scores['recall_score'],2))\n",
        "    val_recall.append(round(valid_scores['recall_score'],2))\n",
        "\n",
        "    metric_results[\"K\"] = kf \n",
        "    metric_results[\"Train ROC-AUC\"] = train_roc\n",
        "    metric_results[\"Val ROC-AUC\"] = val_roc\n",
        "\n",
        "    metric_results[\"Train F1 score\"] = train_F1\n",
        "    metric_results[\"Val F1 score\"] = val_F1\n",
        "\n",
        "    metric_results[\"Train Recall\"] = train_recall\n",
        "    metric_results[\"Val Recall\"] = val_recall\n",
        "\n",
        "    metric_results.to_csv(\"/mydrive/cross_val/cross_val_dag.csv\", index=False)\n",
        "    metric_results = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaXcLxFCME__"
      },
      "source": [
        "## Similarity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7cU60JZMBj2"
      },
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem, DataStructs \n",
        "\n",
        "def similarity(a, b):\n",
        "    if a is None or b is None: \n",
        "        return 0.0\n",
        "    amol = Chem.MolFromSmiles(a)\n",
        "    bmol = Chem.MolFromSmiles(b)\n",
        "    if amol is None or bmol is None:\n",
        "        return 0.0\n",
        "\n",
        "    fp1 = Chem.RDKFingerprint(amol)\n",
        "    fp2 = Chem.RDKFingerprint(bmol)\n",
        "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
        "\n",
        "\n",
        "\n",
        "pred = pd.read_csv(\"/mydrive/drug_discovery/control/gcn_pred.csv\")\n",
        "pred = pred.drop(\"in_trainset\", axis=1)\n",
        "\n",
        "\n",
        "train = pd.read_csv(\"/mydrive/drug_discovery/data.csv\")\n",
        "\n",
        "train_smiles = list(train[\"smiles\"])\n",
        "\n",
        "pred[\"in_trainset\"] = None\n",
        "\n",
        "\n",
        "for row in pred.itertuples():\n",
        "    \n",
        "    for train_mol in train_smiles:\n",
        "        \n",
        "        if similarity(row.smiles, train_mol) == 1:\n",
        "          pred.at[row.Index,'in_trainset'] = True\n",
        "          \n",
        "    if row.in_trainset != True:\n",
        "        pred.at[row.Index,'in_trainset'] = False\n",
        "        \n",
        "pred.to_csv(\"/mydrive/drug_discovery/control/controled/gcn_pred.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWngXldkAdZ6"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTG118GJ9czB"
      },
      "source": [
        "# Load best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVt0kHxFLGeg"
      },
      "source": [
        "### RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DZWrVmwz3rg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78ed4e62-ae6f-43e9-a5dd-fbb40d7ce7a4"
      },
      "source": [
        "data = \"/mydrive/drug_discovery/data.csv\"\n",
        "#test = \"/mydrive/Test/viral_cov-2.csv\"\n",
        "splitter = dc.splits.RandomSplitter()\n",
        "\"\"\"\n",
        "test_acc = []\n",
        "test_F1 = []\n",
        "test_recall = []\n",
        "specif = []\n",
        "sensif = []\n",
        "import pandas as pd\n",
        "metric_results = pd.DataFrame()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntest_acc = []\\ntest_F1 = []\\ntest_recall = []\\nspecif = []\\nsensif = []\\nimport pandas as pd\\nmetric_results = pd.DataFrame()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IuIsc3O8yq2"
      },
      "source": [
        "#data = \"/mydrive/drug_discovery/data.csv\"\n",
        "dataset, transformers = load_data(\"tf\", data)\n",
        "train_dataset, test_dataset = splitter.train_test_split(dataset, frac_train = 0.9, seed=123)\n",
        "#test_dataset, transformers = load_data(\"tf\", test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVJdUKYDLCqG"
      },
      "source": [
        "def model_builder(model_dir):\n",
        "\n",
        "  sklearn_model = RandomForestClassifier(\n",
        "      class_weight = \"balanced\", n_estimators = 300, max_features = 'sqrt', n_jobs=-1)\n",
        "  return dc.models.sklearn_models.SklearnModel(\n",
        "      sklearn_model, model_dir)\n",
        "\n",
        "model = dc.models.multitask.SingletaskToMultitask([1], model_builder)\n",
        "model.fit(train_dataset)\n",
        "\n",
        "y_train = train_dataset.y\n",
        "y_train_pred = model.predict(train_dataset)\n",
        "y_t_pred = np.argmax(y_train_pred[:,0], axis=1) \n",
        "y_train_prediction = np.expand_dims(y_t_pred, -1)\n",
        "\n",
        "y_test = test_dataset.y\n",
        "y_test_pred = model.predict(test_dataset)\n",
        "y_tst_pred = np.argmax(y_test_pred[:,0], axis=1) \n",
        "y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "\n",
        "############################# test ###############################\n",
        "y_test_prediction = y_test_prediction.astype('float64')\n",
        "y_train_prediction = y_train_prediction.astype('float64')\n",
        "\n",
        "\n",
        "metric = dc.metrics.Metric(dc.metrics.f1_score)\n",
        "print('Test set score F1:', model.evaluate(test_dataset, [metric]))\n",
        "print(\"'Test set score MCC:\", matthews_corrcoef(y_test, y_test_prediction))\n",
        "\n",
        "\n",
        "############################ Prediction #########################\n",
        "\n",
        "predictions = y_tst_pred.tolist()\n",
        "my_pred_prob = y_test_pred[:,0].tolist()\n",
        "\n",
        "confidence = []\n",
        "activity = []\n",
        "\n",
        "for index, prob in zip(predictions, my_pred_prob):\n",
        "\n",
        "    confidence.append(round(prob[index], 3))\n",
        "    if index==0:\n",
        "        activity.append(\"inactive\")\n",
        "    else:\n",
        "        activity.append(\"active\")\n",
        "\n",
        "import pandas as pd\n",
        "results = pd.DataFrame()\n",
        "data_cid = pd.read_csv(\"/mydrive/drug_discovery/AID_1409594_smiles.csv\")\n",
        "results[\"PUBCHEM_CID\"] = list(data_cid[\"PUBCHEM_CID\"])\n",
        "results[\"smiles\"] = test_dataset.ids.tolist()\n",
        "results[\"prediction\"] = predictions\n",
        "results[\"activity\"] = activity\n",
        "results[\"confidence\"] = confidence\n",
        "results.to_csv(\"/mydrive/drug_discovery/predict/RF_prediction.csv\", index=False)\n",
        "\n",
        "#################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeoW4x8vpQzf"
      },
      "source": [
        "\"\"\" Roc curve \n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "# predict probabilities\n",
        "lr_probs_rf = model.predict(test_dataset)\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs_rf = lr_probs_rf[:,0]\n",
        "lr_probs_rf = lr_probs_rf[:, 1]\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc_rf = roc_auc_score(y_test, lr_probs_rf)\n",
        "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc_rf))\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr_rf, lr_tpr_rf, _ = roc_curve(y_test, lr_probs_rf)\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(ns_fpr, ns_tpr, linestyle='--')\n",
        "pyplot.plot(lr_fpr_rf, lr_tpr_rf, label='RF', linewidth=2)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" PRC curve \"\"\"\n",
        "# predict probabilities\n",
        "lr_probs_rf = model.predict(test_dataset)\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs_rf = lr_probs_rf[:,0]\n",
        "lr_probs_rf = lr_probs_rf[:, 1]\n",
        "\n",
        "# predict class values\n",
        "yhat = model.predict(testX)\n",
        "yhat = np.argmax(yhat[:,0], axis=1) \n",
        "yhat = np.expand_dims(yhat, -1)\n",
        "\n",
        "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs_rf)\n",
        "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
        "# summarize scores\n",
        "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
        "# plot the precision-recall curves\n",
        "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
        "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--')\n",
        "pyplot.plot(lr_recall, lr_precision, label='RF', linewidth=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9qugXLbLKSa"
      },
      "source": [
        "### GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdwiIRYGLORo"
      },
      "source": [
        "model = dc.models.GCNModel(1,\n",
        "                mode='classification',\n",
        "                batch_size=64,\n",
        "                learning_rate=0.001,\n",
        "                dropout=0.1,\n",
        "                )\n",
        "model.fit(train_dataset, nb_epoch= 40)\n",
        "\n",
        "y_train = train_dataset.y\n",
        "y_train_pred = model.predict(train_dataset)\n",
        "y_t_pred = np.argmax(y_train_pred, axis=1) \n",
        "y_train_prediction = np.expand_dims(y_t_pred, -1)\n",
        "\n",
        "y_test = test_dataset.y\n",
        "y_test_pred = model.predict(test_dataset)\n",
        "y_tst_pred = np.argmax(y_test_pred, axis=1) \n",
        "y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "\n",
        "\n",
        "############################# test ###############################\n",
        "y_test_prediction = y_test_prediction.astype('float64')\n",
        "y_train_prediction = y_train_prediction.astype('float64')\n",
        "\n",
        "\"\"\"\n",
        "metric = dc.metrics.Metric(dc.metrics.f1_score)\n",
        "print('Test set score F1 :', model.evaluate(test_dataset, [metric]))\n",
        "print(\"'Test set score MCC:\", matthews_corrcoef(y_test, y_test_prediction))\n",
        "\"\"\"\n",
        "\n",
        "############################ Prediction #########################\n",
        "\n",
        "predictions = y_tst_pred.tolist()\n",
        "my_pred_prob = y_test_pred.tolist()\n",
        "\n",
        "confidence = []\n",
        "activity = []\n",
        "\n",
        "for index, prob in zip(predictions, my_pred_prob):\n",
        "\n",
        "    confidence.append(round(prob[index], 3))\n",
        "    if index==0:\n",
        "        activity.append(\"inactive\")\n",
        "    else:\n",
        "        activity.append(\"active\")\n",
        "\n",
        "import pandas as pd\n",
        "results = pd.DataFrame()\n",
        "data_cid = pd.read_csv(\"/mydrive/drug_discovery/AID_1409594_smiles.csv\")\n",
        "results[\"PUBCHEM_CID\"] = list(data_cid[\"PUBCHEM_CID\"])\n",
        "results[\"smiles\"] = test_dataset.ids.tolist()\n",
        "results[\"prediction\"] = predictions\n",
        "results[\"activity\"] = activity\n",
        "results[\"confidence\"] = confidence\n",
        "results.to_csv(\"/mydrive/drug_discovery/predict/GCN_prediction.csv\", index=False)\n",
        "\n",
        "##################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi0cruT6xrOH"
      },
      "source": [
        "lr_probs_gcn = model.predict(test_dataset)\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs_gcn = lr_probs_gcn[:, 1]\n",
        "# calculate scores\n",
        "lr_auc_gcn = roc_auc_score(y_test, lr_probs_gcn)\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc_gcn))\n",
        "# calculate roc curves\n",
        "lr_fpr_gcn, lr_tpr_gcn, _ = roc_curve(y_test, lr_probs_gcn)\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(lr_fpr_gcn, lr_tpr_gcn, linewidth=2, label='GCN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avqm1NpoLO2F"
      },
      "source": [
        "### DAG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXaAnsLLSES"
      },
      "source": [
        "default_max_atoms = 60 \n",
        "seed = 123\n",
        "max_atoms_train = max([mol.get_num_atoms() for mol in train_dataset.X])\n",
        "max_atoms_test = max([mol.get_num_atoms() for mol in test_dataset.X])\n",
        "max_atoms = max([max_atoms_train, max_atoms_test])\n",
        "max_atoms = min([max_atoms, default_max_atoms])\n",
        "print('Maximum number of atoms: %i' % max_atoms)\n",
        "reshard_size = 256\n",
        "transformer = dc.trans.DAGTransformer(max_atoms=max_atoms)\n",
        "train_dataset.reshard(reshard_size)\n",
        "train_dataset = transformer.transform(train_dataset)\n",
        "test_dataset.reshard(reshard_size)\n",
        "test_dataset = transformer.transform(test_dataset)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "\n",
        "model = dc.models.DAGModel(\n",
        "            1,\n",
        "            max_atoms=max_atoms,\n",
        "            n_atom_feat=75,\n",
        "            n_graph_feat=30,\n",
        "            n_outputs=30,\n",
        "            batch_size=64,\n",
        "            learning_rate=0.0005,\n",
        "            random_seed=seed,\n",
        "            use_queue=False,\n",
        "            mode='classification')\n",
        "model.fit(train_dataset, nb_epoch= 40)\n",
        "\n",
        "y_train = train_dataset.y\n",
        "y_train_pred = model.predict(train_dataset)\n",
        "y_t_pred = np.argmax(y_train_pred[:,0], axis=1) \n",
        "y_train_prediction = np.expand_dims(y_t_pred, -1)\n",
        "\n",
        "y_test = test_dataset.y\n",
        "y_test_pred = model.predict(test_dataset)\n",
        "y_tst_pred = np.argmax(y_test_pred[:,0], axis=1) \n",
        "y_test_prediction = np.expand_dims(y_tst_pred, -1)\n",
        "\n",
        "\n",
        "############################# test ###############################\n",
        "y_test_prediction = y_test_prediction.astype('float64')\n",
        "y_train_prediction = y_train_prediction.astype('float64')\n",
        "\n",
        "\"\"\"\n",
        "metric = dc.metrics.Metric(dc.metrics.f1_score)\n",
        "print('Test set score F1:', model.evaluate(test_dataset, [metric]))\n",
        "print(\"'Test set score MCC:\", matthews_corrcoef(y_test, y_test_prediction))\n",
        "\"\"\"\n",
        "\n",
        "############################ Prediction #########################\n",
        "\n",
        "predictions = y_tst_pred.tolist()\n",
        "my_pred_prob = y_test_pred[:,0].tolist()\n",
        "\n",
        "confidence = []\n",
        "activity = []\n",
        "\n",
        "for index, prob in zip(predictions, my_pred_prob):\n",
        "\n",
        "    confidence.append(round(prob[index], 3))\n",
        "    if index==0:\n",
        "        activity.append(\"inactive\")\n",
        "    else:\n",
        "        activity.append(\"active\")\n",
        "\n",
        "import pandas as pd\n",
        "results = pd.DataFrame()\n",
        "data_cid = pd.read_csv(\"/mydrive/drug_discovery/AID_1409594_smiles.csv\")\n",
        "results[\"PUBCHEM_CID\"] = list(data_cid[\"PUBCHEM_CID\"])\n",
        "results[\"smiles\"] = test_dataset.ids.tolist()\n",
        "results[\"prediction\"] = predictions\n",
        "results[\"activity\"] = activity\n",
        "results[\"confidence\"] = confidence\n",
        "results.to_csv(\"/mydrive/drug_discovery/predict/DAG_prediction.csv\", index=False)\n",
        "\n",
        "######################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMQjV3adyOmu"
      },
      "source": [
        "lr_probs_dag = model.predict(test_dataset)\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs_dag = lr_probs_dag[:,0]\n",
        "lr_probs_dag = lr_probs_dag[:, 1]\n",
        "# calculate scores\n",
        "lr_auc_dag = roc_auc_score(y_test, lr_probs_dag)\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc_dag))\n",
        "# calculate roc curves\n",
        "lr_fpr_dag, lr_tpr_dag, _ = roc_curve(y_test, lr_probs_dag)\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(lr_fpr_dag, lr_tpr_dag, linewidth=2, label='DAG')\n",
        "\n",
        "\"\"\"\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "pyplot.savefig('/mydrive/drug_discovery/roc/roc_curve_best_models.jpg', format='jpeg', dpi=300)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m27MMZzS4Fag"
      },
      "source": [
        "pyplot.plot(ns_fpr, ns_tpr, linestyle='--')\n",
        "pyplot.plot(lr_fpr_rf, lr_tpr_rf, linewidth=1, label='RF')\n",
        "pyplot.plot(lr_fpr_gcn, lr_tpr_gcn, color='blue', linewidth=1, label='GCN')\n",
        "pyplot.plot(lr_fpr_dag, lr_tpr_dag, linewidth=1, label='DAG')\n",
        "\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "#pyplot.show()\n",
        "pyplot.savefig('/mydrive/drug_discovery/roc/roc_curve_best_models.jpg', format='jpeg', dpi=300)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}